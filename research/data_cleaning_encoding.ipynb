{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " User_ID                      0\n",
      "Age                          0\n",
      "Gender                       0\n",
      "Occupation                   0\n",
      "Country                      0\n",
      "Mental_Health_Condition      0\n",
      "Severity                   501\n",
      "Consultation_History         0\n",
      "Stress_Level                 0\n",
      "Sleep_Hours                  0\n",
      "Work_Hours                   0\n",
      "Physical_Activity_Hours      0\n",
      "dtype: int64\n",
      "   User_ID       Age  Severity  Stress_Level  Sleep_Hours  Work_Hours  \\\n",
      "0        1 -0.421829       2.0             1     0.002569   -0.586396   \n",
      "1        2 -0.565015       NaN             0     0.236077   -0.518376   \n",
      "2        3  1.654381       3.0             0     0.761470    0.229838   \n",
      "3        4 -0.565015       1.0             1     1.578748   -1.674707   \n",
      "4        5 -1.424137       1.0             1    -1.281725    0.501915   \n",
      "\n",
      "   Physical_Activity_Hours  Gender_Female  Gender_Male  Gender_Non-binary  \\\n",
      "0                -0.043503            0.0          0.0                1.0   \n",
      "1                 0.930439            1.0          0.0                0.0   \n",
      "2                 1.579733            0.0          0.0                1.0   \n",
      "3                -1.017444            0.0          1.0                0.0   \n",
      "4                -0.043503            1.0          0.0                0.0   \n",
      "\n",
      "   ...  Country_Canada  Country_Germany  Country_India  Country_Other  \\\n",
      "0  ...             1.0              0.0            0.0            0.0   \n",
      "1  ...             0.0              0.0            0.0            0.0   \n",
      "2  ...             0.0              0.0            0.0            0.0   \n",
      "3  ...             0.0              0.0            0.0            0.0   \n",
      "4  ...             1.0              0.0            0.0            0.0   \n",
      "\n",
      "   Country_UK  Country_USA  Mental_Health_Condition_No  \\\n",
      "0         0.0          0.0                         1.0   \n",
      "1         1.0          0.0                         0.0   \n",
      "2         0.0          1.0                         0.0   \n",
      "3         0.0          0.0                         1.0   \n",
      "4         0.0          0.0                         0.0   \n",
      "\n",
      "   Mental_Health_Condition_Yes  Consultation_History_No  \\\n",
      "0                          0.0                      0.0   \n",
      "1                          1.0                      1.0   \n",
      "2                          1.0                      1.0   \n",
      "3                          0.0                      1.0   \n",
      "4                          1.0                      1.0   \n",
      "\n",
      "   Consultation_History_Yes  \n",
      "0                       1.0  \n",
      "1                       0.0  \n",
      "2                       0.0  \n",
      "3                       0.0  \n",
      "4                       0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Step 1: Load the Data\n",
    "data_path = 'artifacts/data_ingestion/mental_health_dataset.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Inspect and Clean the Data\n",
    "# Check for missing values and handle them if necessary\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# Assume no missing values or anomalies for simplicity; adjust based on actual inspection results\n",
    "\n",
    "# Step 3: Encode Categorical Data\n",
    "# Initialize one-hot encoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "# Fit and transform the data, converting the output to a dense array\n",
    "categorical_columns = ['Gender', 'Occupation', 'Country', 'Mental_Health_Condition', 'Consultation_History']\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(data[categorical_columns]).toarray()\n",
    "# Create DataFrame for one-hot encoded columns\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Map ordinal columns\n",
    "severity_mapping = {'None': 0, 'Low': 1, 'Medium': 2, 'High': 3}\n",
    "stress_level_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "data['Severity'] = data['Severity'].map(severity_mapping)\n",
    "data['Stress_Level'] = data['Stress_Level'].map(stress_level_mapping)\n",
    "\n",
    "# Step 4: Normalize Numerical Data\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Age', 'Sleep_Hours', 'Work_Hours', 'Physical_Activity_Hours']\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Concatenate all processed parts of the dataset\n",
    "processed_data = pd.concat([data.drop(categorical_columns, axis=1), one_hot_df], axis=1)\n",
    "\n",
    "# Display the processed data\n",
    "print(processed_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MentalHealthPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
